{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from urllib.request import urlopen\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import h5py\n",
    "import sys\n",
    "import requests\n",
    "import zipfile\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#file located here: https://drive.google.com/file/d/1tcVYIMcZdlDzGuJvnMtbMchKIK9ulW1P/view\n",
    "\n",
    "#make data directory if doesn't exist in path folder\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "#download meld and mosei zip files for data (mosei is from CMU, meld is friends episodes)\n",
    "if('mosei' in  os.listdir('data')):\n",
    "    pass\n",
    "else:\n",
    "    !file=1tcVYIMcZdlDzGuJvnMtbMchKIK9ulW1P && wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id='${file} -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=\"${file} -O data/data.zip && rm -rf /tmp/cookies.txt\n",
    "\n",
    "    #unzip data\n",
    "    !unzip data/data.zip -d data/mosei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Preprocess our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/eileen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data sentiment statistics\n",
      "3.0    6795\n",
      "4.0    4120\n",
      "2.0    2040\n",
      "5.0    1549\n",
      "1.0    1305\n",
      "Name: sentiment, dtype: int64\n",
      "Test data sentiment statistics\n",
      "3.0    1927\n",
      "4.0    1230\n",
      "2.0     554\n",
      "5.0     441\n",
      "1.0     399\n",
      "Name: sentiment, dtype: int64\n",
      "Validation data sentiment statistics\n",
      "3.0    835\n",
      "4.0    470\n",
      "2.0    244\n",
      "5.0    169\n",
      "1.0    110\n",
      "Name: sentiment, dtype: int64\n",
      "[ 'actually ' 'it ' 'sp ' 'was ' 'sp ' 'probably ' 'the ' 'worst ' 'out ' 'of ' 'all' 'sp ' 'three ' 'sp ' 'and ' 'the ' 'second ' 'one ' `` was n't '' 'that ' 'great' 'either ' 'sp ' ]\n",
      "Number of unique words: 20377\n",
      "{'1.0': 0, '2.0': 1, '3.0': 2, '4.0': 3, '5.0': 4}\n",
      "Words found in wiki vocab: 702\n",
      "New words found: 19675\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from mtl.preprocess import clean_text, create_embedding_matrix, data_processing\n",
    "\n",
    "X_train, X_test, X_train_pad, X_test_pad, y_train, y_test, y_train1, y_test1, y_train2, y_test2, y_train3, y_test3, y_train4, y_test4, y_train5, y_test5, y_train6, y_test6, embedd_matrix, vocab_size, X_val, X_val_pad, y_val, y_val1, y_val2, y_val3, y_val4, y_val5, y_val6, data_test = data_processing()\n",
    "\n",
    "y_mtl_train = (y_train, y_train1, y_train2, y_train3, y_train4, y_train5, y_train6 )\n",
    "y_mtl_val = (y_val, y_val1, y_val2, y_val3, y_val4, y_val5, y_val6)\n",
    "\n",
    "y_emo_train = (y_train1, y_train2, y_train3, y_train4, y_train5, y_train6 )\n",
    "y_emo_val = (y_val1, y_val2, y_val3, y_val4, y_val5, y_val6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Expiriments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Expiriment 1: Baseline LSTM 5 Sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/lstm_one_task_sentiment_five.h5\n",
      "20378\n",
      "Training a lstm_one_task_sentiment_five model!\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 500)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 500, 300)          6113400   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 256)              439296    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " s (Dense)                   (None, 5)                 1285      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,553,981\n",
      "Trainable params: 440,581\n",
      "Non-trainable params: 6,113,400\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "495/495 [==============================] - 192s 385ms/step - loss: 1.4115 - accuracy: 0.4260 - val_loss: 1.3633 - val_accuracy: 0.4568\n",
      "Epoch 2/10\n",
      "495/495 [==============================] - 217s 438ms/step - loss: 1.3775 - accuracy: 0.4304 - val_loss: 1.4029 - val_accuracy: 0.4573\n",
      "Epoch 3/10\n",
      "495/495 [==============================] - 239s 482ms/step - loss: 1.3875 - accuracy: 0.4284 - val_loss: 1.3556 - val_accuracy: 0.4568\n",
      "Epoch 4/10\n",
      "495/495 [==============================] - 234s 473ms/step - loss: 1.3621 - accuracy: 0.4298 - val_loss: 1.3044 - val_accuracy: 0.4644\n",
      "Epoch 5/10\n",
      "495/495 [==============================] - 228s 462ms/step - loss: 1.3357 - accuracy: 0.4380 - val_loss: 1.3002 - val_accuracy: 0.4639\n",
      "Epoch 6/10\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 1.3217 - accuracy: 0.4396 - val_loss: 1.2780 - val_accuracy: 0.4666\n",
      "Epoch 7/10\n",
      "495/495 [==============================] - 223s 451ms/step - loss: 1.3123 - accuracy: 0.4394 - val_loss: 1.2993 - val_accuracy: 0.4546\n",
      "Epoch 8/10\n",
      "495/495 [==============================] - 220s 444ms/step - loss: 1.3108 - accuracy: 0.4439 - val_loss: 1.2741 - val_accuracy: 0.4776\n",
      "Epoch 9/10\n",
      "495/495 [==============================] - 232s 469ms/step - loss: 1.3026 - accuracy: 0.4444 - val_loss: 1.2807 - val_accuracy: 0.4639\n",
      "Epoch 10/10\n",
      "495/495 [==============================] - 234s 472ms/step - loss: 1.2980 - accuracy: 0.4477 - val_loss: 1.2610 - val_accuracy: 0.4765\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "\n",
    "from mtl.network import net\n",
    "from mtl.train import train_model\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "model_name = 'lstm_one_task_sentiment_five'\n",
    "saved_model_name = 'models/'+ model_name + '.h5'\n",
    "print('models/'+ model_name + '.h5')\n",
    "batch_size=32\n",
    "epochs=10\n",
    "gru_output_size=128\n",
    "dropout=0.2\n",
    "recurrent_dropout=0.2\n",
    "tensorboard = True\n",
    "loss_weights = [0.4, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
    "print(vocab_size)\n",
    "\n",
    "\n",
    "train_model(vocab_size = vocab_size, embedd_matrix = embedd_matrix, data_x = X_train_pad, data_y = y_train, val_x = X_val_pad, val_y = y_val, model_name=model_name, saved_model_name=saved_model_name, batch_size=32, epochs=epochs, tensorboard=tensorboard, loss_weights=loss_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 44.06%\n",
      "\n",
      "F1 Score: 44.06\n"
     ]
    }
   ],
   "source": [
    "#predict\n",
    "from mtl.predict import test_model\n",
    "\n",
    "file_name = 'models/lstm_one_task_sentiment_five.h5'\n",
    "num_classes = 5\n",
    "num_classes1 = 2\n",
    "embed_num_dims = 300\n",
    "max_seq_len = 500\n",
    "\n",
    "#change those according to the model you are testing\n",
    "num_of_sentiments = True\n",
    "\n",
    "test_model(file_name, mtl = False, data_y= data_test, data_x=X_test_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=logs/fit --host localhost --port 6006\n",
    "\n",
    "#then open browser and go to: http://localhost:6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Expiriment Two: Baseline GRU 5 Sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gru_one_task_sentiment_five.h5\n",
      "20378\n",
      "Training a gru_one_task_sentiment_five model!\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 500)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 500, 300)          6113400   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 256)              330240    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " s (Dense)                   (None, 5)                 1285      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,444,925\n",
      "Trainable params: 331,525\n",
      "Non-trainable params: 6,113,400\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "495/495 [==============================] - 219s 438ms/step - loss: 1.4113 - accuracy: 0.4301 - val_loss: 1.3585 - val_accuracy: 0.4568\n",
      "Epoch 2/10\n",
      "495/495 [==============================] - 231s 467ms/step - loss: 1.3643 - accuracy: 0.4342 - val_loss: 1.3525 - val_accuracy: 0.4519\n",
      "Epoch 3/10\n",
      "495/495 [==============================] - 233s 471ms/step - loss: 1.3351 - accuracy: 0.4410 - val_loss: 1.2902 - val_accuracy: 0.4672\n",
      "Epoch 4/10\n",
      "495/495 [==============================] - 224s 452ms/step - loss: 1.3229 - accuracy: 0.4431 - val_loss: 1.2804 - val_accuracy: 0.4765\n",
      "Epoch 5/10\n",
      "495/495 [==============================] - 203s 410ms/step - loss: 1.3091 - accuracy: 0.4455 - val_loss: 1.2646 - val_accuracy: 0.4759\n",
      "Epoch 6/10\n",
      "495/495 [==============================] - 211s 427ms/step - loss: 1.2978 - accuracy: 0.4460 - val_loss: 1.2517 - val_accuracy: 0.4754\n",
      "Epoch 7/10\n",
      "495/495 [==============================] - 256s 517ms/step - loss: 1.2889 - accuracy: 0.4517 - val_loss: 1.2501 - val_accuracy: 0.4787\n",
      "Epoch 8/10\n",
      "495/495 [==============================] - 214s 432ms/step - loss: 1.2795 - accuracy: 0.4537 - val_loss: 1.2499 - val_accuracy: 0.4721\n",
      "Epoch 9/10\n",
      "495/495 [==============================] - 203s 411ms/step - loss: 1.2740 - accuracy: 0.4602 - val_loss: 1.2472 - val_accuracy: 0.4765\n",
      "Epoch 10/10\n",
      "495/495 [==============================] - 212s 429ms/step - loss: 1.2675 - accuracy: 0.4594 - val_loss: 1.2470 - val_accuracy: 0.4858\n"
     ]
    }
   ],
   "source": [
    "from mtl.network import net\n",
    "from mtl.train import train_model\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "model_name = 'gru_one_task_sentiment_five'\n",
    "saved_model_name = 'models/'+ model_name + '.h5'\n",
    "print('models/'+ model_name + '.h5')\n",
    "batch_size=32\n",
    "epochs=10\n",
    "gru_output_size=128\n",
    "dropout=0.2\n",
    "recurrent_dropout=0.2\n",
    "tensorboard = True\n",
    "loss_weights = [0.4, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
    "print(vocab_size)\n",
    "\n",
    "\n",
    "train_model(vocab_size = vocab_size, embedd_matrix = embedd_matrix, data_x = X_train_pad, data_y = y_train, val_x = X_val_pad, val_y = y_val, model_name=model_name, saved_model_name=saved_model_name, batch_size=32, epochs=epochs, tensorboard=tensorboard, loss_weights=loss_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 44.21%\n",
      "\n",
      "F1 Score: 44.21\n"
     ]
    }
   ],
   "source": [
    "#predict\n",
    "from mtl.predict import test_model\n",
    "\n",
    "file_name = 'models/lstm_one_task_sentiment_five.h5'\n",
    "num_classes = 5\n",
    "num_classes1 = 2\n",
    "embed_num_dims = 300\n",
    "max_seq_len = 500\n",
    "\n",
    "#change those according to the model you are testing\n",
    "num_of_sentiments = True\n",
    "\n",
    "test_model(file_name, mtl = False, data_y= data_test, data_x=X_test_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 16:56:32.407902: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "TensorBoard 2.7.0 at http://localhost:6006/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=logs/fit --host localhost --port 6006\n",
    "\n",
    "#then open browser and go to: http://localhost:6006"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
