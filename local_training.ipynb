{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from urllib.request import urlopen\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import h5py\n",
    "import sys\n",
    "import requests\n",
    "import zipfile\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#file located here: https://drive.google.com/file/d/1tcVYIMcZdlDzGuJvnMtbMchKIK9ulW1P/view\n",
    "\n",
    "#make data directory if doesn't exist in path folder\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "#download meld and mosei zip files for data (mosei is from CMU, meld is friends episodes)\n",
    "if('mosei' in  os.listdir('data')):\n",
    "    pass\n",
    "else:\n",
    "    !file=1tcVYIMcZdlDzGuJvnMtbMchKIK9ulW1P && wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id='${file} -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=\"${file} -O data/data.zip && rm -rf /tmp/cookies.txt\n",
    "\n",
    "    #unzip data\n",
    "    !unzip data/data.zip -d data/mosei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Preprocess our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/eileen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data sentiment statistics\n",
      "3.0    6795\n",
      "4.0    4120\n",
      "2.0    2040\n",
      "5.0    1549\n",
      "1.0    1305\n",
      "Name: sentiment, dtype: int64\n",
      "Test data sentiment statistics\n",
      "3.0    1927\n",
      "4.0    1230\n",
      "2.0     554\n",
      "5.0     441\n",
      "1.0     399\n",
      "Name: sentiment, dtype: int64\n",
      "Validation data sentiment statistics\n",
      "3.0    835\n",
      "4.0    470\n",
      "2.0    244\n",
      "5.0    169\n",
      "1.0    110\n",
      "Name: sentiment, dtype: int64\n",
      "[ 'input ' 'sp ' 'but ' 'even ' 'if ' 'the ' 'application ' `` is n't '' 'written' 'well ' 'having ' 'this ' 'web ' 'application ' 'firewall ' 'gives ' 'you' 'another ' 'line ' 'of ' 'defense ' 'sp ' 'you ' 'may ' ]\n",
      "Number of unique words: 20377\n",
      "{'1.0': 0, '2.0': 1, '3.0': 2, '4.0': 3, '5.0': 4}\n",
      "Words found in wiki vocab: 702\n",
      "New words found: 19675\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from mtl.preprocess import clean_text, create_embedding_matrix, data_processing\n",
    "\n",
    "X_train, X_test, X_train_pad, X_test_pad, y_train, y_test, y_train1, y_test1, y_train2, y_test2, y_train3, y_test3, y_train4, y_test4, y_train5, y_test5, y_train6, y_test6, embedd_matrix, vocab_size, X_val, X_val_pad, y_val, y_val1, y_val2, y_val3, y_val4, y_val5, y_val6, data_test = data_processing()\n",
    "\n",
    "y_mtl_train = (y_train, y_train1, y_train2, y_train3, y_train4, y_train5, y_train6 )\n",
    "y_mtl_val = (y_val, y_val1, y_val2, y_val3, y_val4, y_val5, y_val6)\n",
    "\n",
    "y_emo_train = (y_train1, y_train2, y_train3, y_train4, y_train5, y_train6 )\n",
    "y_emo_val = (y_val1, y_val2, y_val3, y_val4, y_val5, y_val6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Expiriments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Expiriment 1: Baseline Model LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/lstm_one_task_sentiment_five.h5\n",
      "20378\n",
      "Training a lstm_one_task_sentiment_five model!\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 500)]             0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 500, 300)          6113400   \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              439296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " s (Dense)                   (None, 5)                 1285      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,553,981\n",
      "Trainable params: 440,581\n",
      "Non-trainable params: 6,113,400\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "495/495 [==============================] - 187s 373ms/step - loss: 1.4097 - accuracy: 0.4295 - val_loss: 1.3412 - val_accuracy: 0.4590\n",
      "Epoch 2/2\n",
      "495/495 [==============================] - 217s 439ms/step - loss: 1.3677 - accuracy: 0.4320 - val_loss: 1.3227 - val_accuracy: 0.4655\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "\n",
    "from mtl.network import net\n",
    "from mtl.train import train_model\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "model_name = 'lstm_one_task_sentiment_five'\n",
    "saved_model_name = 'models/'+ model_name + '.h5'\n",
    "print('models/'+ model_name + '.h5')\n",
    "batch_size=32\n",
    "epochs=2\n",
    "gru_output_size=128\n",
    "dropout=0.2\n",
    "recurrent_dropout=0.2\n",
    "tensorboard = True\n",
    "loss_weights = [0.4, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
    "print(vocab_size)\n",
    "\n",
    "\n",
    "train_model(vocab_size = vocab_size, embedd_matrix = embedd_matrix, data_x = X_train_pad, data_y = y_train, val_x = X_val_pad, val_y = y_val, model_name=model_name, saved_model_name=saved_model_name, batch_size=32, epochs=epochs, tensorboard=tensorboard, loss_weights=loss_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict: DOES NOT WORK YET\n",
    "from mtl.predict import test_model\n",
    "\n",
    "file_name = 'models/mtl_five_sentiments20220401-122307.h5'\n",
    "num_classes = 7\n",
    "num_classes1 = 2\n",
    "embed_num_dims = 300\n",
    "max_seq_len = 500\n",
    "\n",
    "#change those according to the model you are testing\n",
    "mtl = True\n",
    "num_of_sentiments = True\n",
    "\n",
    "test_model(file_name, mtl, data_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
