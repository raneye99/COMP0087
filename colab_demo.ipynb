{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNHAlmOKzbWKYIvAQfCa3b/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raneye99/COMP0087/blob/main/colab_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone Git Repo into Google Drive\n",
        "\n",
        "Note: you will need to create a personal access token and store it into the directory where you will clone the repo."
      ],
      "metadata": {
        "id": "CkKV6L_t7OvS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBdyhbwbl167",
        "outputId": "c9d5a7e5-1d71-42d0-9433-cb809de16d06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "changing directory to: /content/drive/MyDrive/NLP/COMP0087\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "Already up to date.\n",
            "\n",
            "adding /content/drive/MyDrive/NLP/COMP0087 to sys.path\n"
          ]
        }
      ],
      "source": [
        "#set up\n",
        "gdrive_location = \"/content/drive/MyDrive/NLP\"\n",
        "pat_file = \"pat.json\"\n",
        "import subprocess\n",
        "import json\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "\n",
        "\n",
        "gdrive_mount = '/content/drive'\n",
        "# requires giving access to google drive account\n",
        "drive.mount(gdrive_mount)\n",
        "\n",
        "# change to relevant workspace \n",
        "# - google drive dir - where 'pat_file' should be found\n",
        "gdrive_dir = os.path.join(gdrive_mount, gdrive_location)\n",
        "work_dir = gdrive_dir\n",
        "\n",
        "#url for repo\n",
        "url = \":x-oauth-basic@github.com/raneye99/COMP0087\"\n",
        "\n",
        "#repo directory\n",
        "repo_dir = os.path.join(gdrive_dir, os.path.basename(url))\n",
        "\n",
        "# change to working directory\n",
        "# os.chdir(work_dir)\n",
        "assert os.path.exists(work_dir), f\"workspace directory: {work_dir} does not exist\"\n",
        "os.chdir(work_dir)\n",
        "# this is a bit long - would be good to reduce\n",
        "# if git directory does not exist - clone \n",
        "if not os.path.exists(repo_dir):\n",
        "    # get pat file - expected to be in work_dir\n",
        "    # pat expected to be dict: {\"pat\": \"<personal access token>\"}\n",
        "    with open(os.path.join(gdrive_dir, pat_file), \"r+\") as f:\n",
        "        pat = json.load(f)\n",
        "\n",
        "    # git clone\n",
        "    print(f\"cloning directory: {url}\")\n",
        "    git_clone = subprocess.check_output( [\"git\", \"clone\", f\"https://{pat['pat']}{url}\"] , shell=False)\n",
        "    print(git_clone)\n",
        "\n",
        "print(f\"changing directory to: {repo_dir}\")\n",
        "os.chdir(repo_dir)\n",
        "\n",
        "# --\n",
        "# change branch - review this\n",
        "# --\n",
        "\n",
        "try:\n",
        "    branch_name = \"main\"\n",
        "    git_checkout = subprocess.check_output([\"git\", \"checkout\", \"-t\", f\"origin/{branch_name}\"], shell=False)\n",
        "    print(git_checkout.decode(\"utf-8\") )\n",
        "except Exception as e:\n",
        "    git_checkout = subprocess.check_output([\"git\", \"checkout\",  f\"{branch_name}\"], shell=False)\n",
        "    print(git_checkout.decode(\"utf-8\") )\n",
        "\n",
        "# ---\n",
        "# git pull\n",
        "# ---\n",
        "# could be redunant if just clone, but whatever\n",
        "\n",
        "git_pull = subprocess.check_output([\"git\", \"pull\"], shell=False)\n",
        "print(git_pull.decode(\"utf-8\") )\n",
        "\n",
        "\n",
        "if repo_dir not in sys.path:\n",
        "    print(f\"adding {repo_dir} to sys.path\")\n",
        "    sys.path.extend([repo_dir])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "from urllib.request import urlopen\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import h5py\n",
        "import sys\n",
        "import requests\n",
        "import zipfile\n",
        "import inspect"
      ],
      "metadata": {
        "id": "ALYAb53bticz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Data"
      ],
      "metadata": {
        "id": "z3ur0_jl7fCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load data\n",
        "\n",
        "#file located here: https://drive.google.com/file/d/1tcVYIMcZdlDzGuJvnMtbMchKIK9ulW1P/view\n",
        "\n",
        "#make data directory if doesn't exist in path folder\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "#download meld and mosei zip files for data (mosei is from CMU, meld is friends episodes)\n",
        "if('mosei' in  os.listdir('data')):\n",
        "    pass\n",
        "else:\n",
        "    !file=1tcVYIMcZdlDzGuJvnMtbMchKIK9ulW1P && wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id='${file} -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=\"${file} -O data/data.zip && rm -rf /tmp/cookies.txt\n",
        "\n",
        "    #unzip data\n",
        "    !unzip data/data.zip -d data/mosei"
      ],
      "metadata": {
        "id": "YS610PdruF33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess our data"
      ],
      "metadata": {
        "id": "9fJZ-QvQ7g_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocess data\n",
        "from mtl.preprocess import clean_text, create_embedding_matrix, data_processing\n",
        "\n",
        "X_train, X_test, X_train_pad, X_test_pad, y_train, y_test, y_train1, y_test1, y_train2, y_test2, y_train3, y_test3, y_train4, y_test4, y_train5, y_test5, y_train6, y_test6, embedd_matrix, vocab_size, X_val, X_val_pad, y_val, y_val1, y_val2, y_val3, y_val4, y_val5, y_val6, data_test = data_processing()\n",
        "\n",
        "y_mtl_train = (y_train, y_train1, y_train2, y_train3, y_train4, y_train5, y_train6 )\n",
        "y_mtl_val = (y_val, y_val1, y_val2, y_val3, y_val4, y_val5, y_val6)\n",
        "\n",
        "y_emo_train = (y_train1, y_train2, y_train3, y_train4, y_train5, y_train6 )\n",
        "y_emo_val = (y_val1, y_val2, y_val3, y_val4, y_val5, y_val6)\n"
      ],
      "metadata": {
        "id": "4zSashf3uYT8",
        "outputId": "9a564baa-5ff9-4d0a-8fa7-d83053e47c7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Training data sentiment statistics\n",
            "4.0    4120\n",
            "3.0    3000\n",
            "2.0    2040\n",
            "5.0    1549\n",
            "1.0    1305\n",
            "Name: sentiment, dtype: int64\n",
            "Test data sentiment statistics\n",
            "3.0    1927\n",
            "4.0    1230\n",
            "2.0     554\n",
            "5.0     441\n",
            "1.0     399\n",
            "Name: sentiment, dtype: int64\n",
            "Validation data sentiment statistics\n",
            "3.0    835\n",
            "4.0    470\n",
            "2.0    244\n",
            "5.0    169\n",
            "1.0    110\n",
            "Name: sentiment, dtype: int64\n",
            "[ 'website ' 'sp ' 'at ' 'sp ' 'if ' 'you ' 'get ' 'any ' 'other ' 'cool ' 'ideas' 'like ' 'susan ' 'williams ' 'shared ' 'with ' 'me ' 'come ' 'on ' 'and ' 'share' 'them ' 'with ' 'me ' `` i 'll '' 'be ' 'happy ' 'to ' ]\n",
            "Number of unique words: 18315\n",
            "{'1.0': 0, '2.0': 1, '3.0': 2, '4.0': 3, '5.0': 4}\n",
            "Words found in wiki vocab: 653\n",
            "New words found: 17662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Expiriments"
      ],
      "metadata": {
        "id": "BIA9d6p37i2T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Expiriment One: Baseline LSTM with 5 sentiments"
      ],
      "metadata": {
        "id": "CaQCQ3ub7l_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train"
      ],
      "metadata": {
        "id": "9LL76DFa7sVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#train\n",
        "\n",
        "from mtl.network import net\n",
        "from mtl.train import train_model\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "\n",
        "model_name = 'lstm_one_task_sentiment_five'\n",
        "saved_model_name = 'models/'+ model_name + '.h5'\n",
        "print('models/'+ model_name + '.h5')\n",
        "batch_size=32\n",
        "epochs=10\n",
        "gru_output_size=128\n",
        "dropout=0.2\n",
        "recurrent_dropout=0.2\n",
        "tensorboard = True\n",
        "loss_weights = [0.4, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
        "print(vocab_size)\n",
        "\n",
        "\n",
        "train_model(vocab_size = vocab_size, embedd_matrix = embedd_matrix, data_x = X_train_pad, data_y = y_train, val_x = X_val_pad, val_y = y_val, model_name=model_name, saved_model_name=saved_model_name, batch_size=32, epochs=epochs, tensorboard=tensorboard, loss_weights=loss_weights)"
      ],
      "metadata": {
        "id": "-uY7FuM7nY5A",
        "outputId": "27c373d0-965b-484d-b4d1-eb0819987489",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/mtl_five_sentiments20220404-114111.h5\n",
            "18316\n",
            "Training a mtl_five_sentiments model!\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 500)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 500, 300)     5494800     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 256)          330240      ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " s (Dense)                      (None, 5)            1285        ['bidirectional[0][0]']          \n",
            "                                                                                                  \n",
            " e1 (Dense)                     (None, 2)            514         ['bidirectional[0][0]']          \n",
            "                                                                                                  \n",
            " e2 (Dense)                     (None, 2)            514         ['bidirectional[0][0]']          \n",
            "                                                                                                  \n",
            " e3 (Dense)                     (None, 2)            514         ['bidirectional[0][0]']          \n",
            "                                                                                                  \n",
            " e4 (Dense)                     (None, 2)            514         ['bidirectional[0][0]']          \n",
            "                                                                                                  \n",
            " e5 (Dense)                     (None, 2)            514         ['bidirectional[0][0]']          \n",
            "                                                                                                  \n",
            " e6 (Dense)                     (None, 2)            514         ['bidirectional[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5,829,409\n",
            "Trainable params: 334,609\n",
            "Non-trainable params: 5,494,800\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/2\n",
            " 88/376 [======>.......................] - ETA: 15:24 - loss: 0.9075 - s_loss: 1.5249 - e1_loss: 0.6845 - e2_loss: 0.5791 - e3_loss: 0.5226 - e4_loss: 0.3739 - e5_loss: 0.5119 - e6_loss: 0.3038 - s_accuracy: 0.3558 - e1_accuracy: 0.5703 - e2_accuracy: 0.7365 - e3_accuracy: 0.7947 - e4_accuracy: 0.8874 - e5_accuracy: 0.7997 - e6_accuracy: 0.9197"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predict"
      ],
      "metadata": {
        "id": "hiE41XF47uJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#predict\n",
        "from mtl.predict import test_model\n",
        "\n",
        "file_name = 'models/lstm_one_task_sentiment_five.h5'\n",
        "num_classes = 5\n",
        "num_classes1 = 2\n",
        "embed_num_dims = 300\n",
        "max_seq_len = 500\n",
        "\n",
        "#change those according to the model you are testing\n",
        "num_of_sentiments = True\n",
        "\n",
        "test_model(file_name, mtl = False, data_y= data_test, data_x=X_test_pad)"
      ],
      "metadata": {
        "id": "HQkUG95u7GL4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}